\documentclass[11pt,a4paper,twocolumn]{book}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{booktabs}
\author{Ege Ã–zkan}
\title{CENG 311 \\ \large{Computer Architecture Lecture Notes}}
\begin{document}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\cputime}{\ensuremath{\text{CPU Time}}}
\newcommand{\cyclecount}{\text{Cycle Count}}
\newcommand{\cycletime}{\text{Cycle Time}}
\newcommand{\clockrate}{\text{Clock Rate}}

\newcommand{\C}[1]{\code{#1}}

\newcommand{\rs}[1]{\code{\$s#1}} % an s register.
\newcommand{\rt}[1]{\code{\$t#1}} % a t register
\newcommand{\Rs}[1]{\code{\$s#1}}
\newcommand{\Rt}[1]{\code{\$t#1}}
\newcommand{\Ra}[1]{\code{\$a#1}}
\newcommand{\Rv}[1]{\code{\$v#1}}
\newcommand{\Rra}{\code{\$ra}}
\newcommand{\Rsp}{\code{\$sp}}
\maketitle
\newcommand{\inscount}{\text{IC}}
\newcommand{\missed}{\textit{!*}}
\lstdefinelanguage
   [MISP]{Assembler}     % add a "x64" dialect of Assembler
   [x86masm]{Assembler} % based on the "x86masm" dialect
   % with these extra keywords:
   {morekeywords={lw, sw, \$t0, \$t1,
   \$t2, \$t3, \$t4, \$t5, \$t6, \$t7,
   \$t8, \$t9, \$s0, \$s1, \$s2, \$s3,
   \$s4, \$s5, \$s6, \$s7, \$s8, \$s9,
   add, mul, sub, div,
   li, addi, muli, bne, beq, j,
   \$a0, \$a1, \$a2, \$a3, \$v0, \$v1,
   \$sp, \$ra}} % etc.


\lstset{language=[MISP]Assembler}

\chapter{Introduction - October 16, 2020}

\section{Four Key Current Directions}

\begin{enumerate}
\item Fundementally secure/reliable/safe architectuers
\item Fundementally energy-efficent and memory centric architectures
\item Fundementally low latency and predictable archiectures
\item Architectures for AI/ML, Genomics, Medicine, etc.
\end{enumerate}

\section{Transformation Hierarchy}

The order travels through different hierarchical levels until it reaches the electrons. From problems to algorithms, thrugh program/language to system software to SW/HW Interface to lower hardware components.\\

Computer architecture was traditionally limited to SW/HW Interface and to Micro-architecture, but in the present day, computer architecture expands from algorithms to devices. This is because, to achieve the highest energy efficency and performance, one must take the expanded view therefore co-designing across the hierarchy.\\

This way, once can specialize most of the components for a specific domain.

\section{Computer Architecture}

Computer architecture is the science and art of designin computing platforms to achibe a set of design goals. Designing a supercomputer is different from designing a smartphone, but many fundemental principles are similiar.\\

The computer architecture allows better systems to be built by making computer faster, cheaper, smaller and more reliable, it enables new applications and enables better solutions to be found.\\

Studying computer architecutre allows one to understand why computers work the way they do.

\subsection{Computer Architecture Today}

The present day industry has entered a paradigm shift to novel architectures, as many difficult problems motivate and cause a demand for novel architectures.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% CHAPTER II %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Computer Abstractions - October 23, 2020}


Computer architecture is the science and art of selecting and interconnecting hardware components to create a computer that meets functional, performance and cost goals. Computer systems come in many  forms, from general purpose personal computers to supercomputers.\\

A system must be \textit{Functional} (correct), \textit{Reliable} (continue to perform correctly.), High performance, low cost, low power/energy consumption and it must be secure. Keep in mind that the word \textit{correct} may mean different things in different accuracy levels.\\

\section{Abstractions}

The computer system structure consists of the Application software at the top, the system software in the middle and the hardware in the bottom. \textbf{Hardware-software interface} handles the translation.\\

The compiler takes a high level compiled language (such as C), converts it into an Assembly language, and the assembler takes the Assembler code and converts it into the machine code.\\

A Microarchitecture is an impleentation of an ISA, there may be multiple implementations from the same ISA.\\

Levels of transmformation create abstractions. High-level language programmer does not really need to know what the ISA is and how a computer executes instructions. Abstractions also improve productivity, decisions made in the underlaying level does not need to be considered.\\

Knowledge of the lower level may improve higher level design choices of a person. [For instance, consider the fact that modulo operator is \textit{slower} in the hardware level, when writing C code, modulo operations can be transformed into bit-shift operations, which are \textit{faster}, improving speed.]

\section{the Von Neumann Architecture}

the Von Neumann Architecture consists of a main memory, a CPU and the interconnection between them. Within the CPU, there is a control unit, and an arithmatic/logic unit. the Von Neumann Architecture is the \textit{traditional} structure of computers.\\

\section{Main Memory}

Collection of locations, each of which is capable of storing both instructions and data. Every location consists of an adress, which is used to access the location, and the contents of the location. It is similar in structure to a programming array. [Intresting to note, many emulators actually use arrays to emulate the memory of simpler machines.]

\section{Central Processing Unit}

CPU consists of multiple parts, \textbf{control unit}, \textbf{arithmatic logic unit} and \textbf{registers}. Memory is \textit{fetched/read} to the CPU, as CPU sizes tend to be significantly smaller then Memory sizes, that is why despite CPU being much faster, memory is used to read/fetch from and \textit{write/store} to.

\subsection{Program Compilation as Execution}

C is a compiled language, which means C code such as \code{float value = x[i];} would get translated to assembly instructions such as \code{ld r0 addr[1]}. [Many C compilers also optimize your code, so there are changes made to it. (Some even causing bugs.)]\\

The resulting assembly code is stored in the memory as binary machine code. These instructions are fetched from the memory to the CPU (ALU). A specialised register called \textbf{program counter} (PC), \textit{points} to the instruction to be executed. When ALU executes an instruction, program counter is incremented.\\

\subsection{Metrics}

Performance is generally valued in the CPU architecture, this is not as straightforward as one assumes, however. Consider an airplane, airplane performance can be defined as passsanger capacity, cruising speed, crusining range or passangers per mph. As such, there are different \textit{metrics} to optimize for.\\

For computers, \textbf{Latency} is the elapsed time to do a task, how long it takes to do a task while \textbf{Throughput} is total work done per unit time. Although they may \textit{look} similar, they are not the same.\\

In this course, while discussing processor performance, the focus will primarilly be on the execution time for a single job (latency). It can be measured in different ways, \textbf{Execution time} includes all aspects, the total response time while \textbf{CPU time} is the direct time spent on processing a given job, this discounts IO time and other jobs' shares.

\begin{equation}
\text{Performance} = \frac{1}{\text{Execution Time}}
\end{equation}

Relative performance is the performance comperasion between two computers. Saying X is $n$ time faster than Y is the same as dividing their performances or the inverse of their execution times.\\

If Computer A runs a program in 10 seconds, and B in 15 seconds, A is told to be 1.5 times faster than B.

\subsection{CPU Clock}

CPU clock determines when events take place in the hardware. [a clock cycle is similar to a Minecraft Tick.] a clock period is the duration of a clock cycle. Clock period has the unit of time and Clock frequency, which is the cycles per second, has the unit of Hz.\\

This has the effect that the CPU Time is the:

\begin{equation}
\cputime = \cyclecount \times \cycletime 
\end{equation}

Where $\cyclecount$ count of cycles needed for program to run, $\cputime$ is the CPU time of the program, $\cycletime$ is the time it takes for a single CPU cycle.

\begin{equation}
\cputime = \frac{\cyclecount}{\clockrate}
\end{equation}

Where $\clockrate$ is the clock rate. Therefore, increasing the clock rate, or decreasing the cycle time will improve performance.\\

For isntance, if a program runs in 10 seconds on a computer A, which has a 2GHz clock rate, whihc means the clock cycles needed for this program in computer A is $2 \times 10^9\text{Hz} \times 10\text{s} $ Clock cycles.\\

If computer B runs this program in 6 seconds, but 1.2x more clock cycles, to calculate its clock rate, one just has to multiply the Clock rate above with $\frac{1.2}{6\text{s}}$, which results in the calculation $\clockrate = 4$GHz\\

Different instructions take different amounts of time on different machines, division generally takes more time than addtion, floating point operations take longer than itneger ones, accessing memory takes more time than accessing registers. So one can assume that the number of cycles will equal the number of instructions for simplicity, but it would be incorrect.\\

Instruction count for a program is determined by a program, ISA and compiler, but the avarage cycles per instruction is determined by CPU hardware, if different instructions have different CPI, avarage CPI is affected by instruction mix.\\

Clock Cycles equal Instruction count $\times$ Cycles per instruction, whereas the CPU time depends on the product of Instruction count, CPI and Clock Cycle time, keep in mind Clock Cycle Time can be changed with $\frac{1}{\clockrate}$.\\

Imagine an example where:
\begin{table}[ht]
\begin{tabular}{lll}
Computer & Cycle Time & CPI\\
\toprule
A & 250ps & 2.0\\
B & 500ps & 1.2\\
\bottomrule
\end{tabular}
\end{table}

Here, $t_a = \text{CC} \times \text{CCT} = I \times 2 \times 250$ and $t_b = I \times 1.2 \times 500$ where $I$ is the instruction count, comparing them, $\frac{t_a}{t_b} = \frac{500I}{600I}$, shows that computer a is $\frac{6}{5}$ times faster for a program.\\

Another example, this time with compilers:

\begin{table}[ht]
\begin{tabular}{lccc}
 & A & B & C\\
\toprule
CPI for class & 1 & 2 & 3\\
IC in sequence 1 & 2 & 1 & 2\\
IC in sequence 2 & 4 & 1 & 1\\
\bottomrule
\end{tabular}
\end{table}

Here the compiler writer can chose between different code generation sequences for three classes A, B and C. But which code sequence is faster, and what is the CPI for each sequence?\\

For the first sequence, there is a total of $1 \times 2 + 2 \times 1 + 3 \times 2 = 10$ Clock Cycles spent, for the second sequence, there is a total of $1 \times 4 + 2 \times 1 + 3 \times 1 = 9$. Since the compiler will execute them in the same computer, the clock rate is equal, and hence, the time being spent is directly related to the number of clock cycles, therefore, the second sequence is much more beneficial.\\

One could also calulcate the avarage CPI, this is calculated using:

\begin{align}
\text{CPI}_A &= \frac{\text{Clock Cycles}}{\text{Instruction Count}}\\
&= \sum_{i = 1}^n \left( \text{CPI}_i \times \frac{\text{Instruction Count}_i}{\text{Instruction Count}}\right)\\
\text{Clock Cycles} &= \sum_{i =1}^n \left( \text{CPI}_i \times \text{Instruction Count}_i \right)
\end{align}

Where $\text{CPI}_A$ is the weighted avarage CPI.

Overall:

\begin{equation}
\text{CPU Time} = \frac{\text{Instructions}}{\text{Program}} \times \frac{\text{Clock cycles}}{\text{Instruction}} \times \frac{\text{Seconds}}{\text{Clock Cycle}}
\end{equation}

\subsubsection{Determining the Values}

\begin{description}
\item[CPU Execution Time] can be determined by running the program.
\item[Clock cycle time] is usually published with documentation.
\item[Instruction Count] via the software tools that profile execution or by simulators.
\item[CPI] however, depends on a wide variety of design details including the memory system and the processor structure, therefore it is much harder to determine.
\end{description}

\textbf{Computer Benchmarks} are programs or set of programs used to evaluate computer performance, benchmarks allow us to make performance comparisions based on eexecution times, they can vary greatly in terms of their complexity and their usefulness, benchmarks should:

\begin{itemize}
\item Be representitive of the type of applications that run on the computer.
\item Not be overly dependent on one or two features of a computer.
\end{itemize}

\subsubsection{Amdahl's Law}

Improving an aspect of a computr and expecting a proportional improvement in overall performance. Where $n$ is the improvement factor:

\begin{equation}
T_{\text{improved}} = \frac{T_{affected}}{n} + T_{\text{unaffected}}
\end{equation}

For instance if multiply accounts for 80 seconds of 100, how much improvement in multiply perfromance to get 2 times overall improvement. [ie to make 100 seconds to fall to 50 seconds]

\begin{equation}
50 = \frac{80}{n} + 20
\end{equation}

Must imrpove by 2.6 times.

However if multiply accounts for 80 seconds of 100 seconds and we want five times overall improvement. This is impossible, since the unaffected part also takes 20 seconds.

\subsubsection{Summary of Performance Evaluation}

Good benchmarks, such as SPEC can provide an accurate method for evaluating and comparing computer performance. Ahmdal's law provides an efficent method for determining speedup due to an enhancment, and one must make the common case fast.

\chapter{Instructions: Language of the Computer - October 30, 2020}

It was allready established that the Compilers and Assemblers bring high-level languages to binary machine language program. The Instruction Set Architecture (ISA) consists of Instructions and Memory.\\

ISA's instructions are controlled by Opcodes, Addressing Modes, Data Types, Instruction Types and Fromats, Registers and Condition codes. An ISA interracts with the memory's address space, it has to deal with its addressability, alignment and virtual memory managment.\\

A microarchitecture is a specific implementation of an ISA under specific design constraints and goals. It is the things done in hardware without exposure to the software. These range from pipelining to voltage/frequency scaling.

\section{MIPS Instruction Set}

MIPS is a simple ISA.

\subsection{Assembler Instructions}

\subsubsection{Add and Substract}

Takes two sources and has one destination.

\begin{lstlisting}
add a, b, c
\end{lstlisting}

Where \code{a} gets \code{b + c}, arithmatic operations occur \textbf{only on registers} in MIPS. \code{sub}, \code{mult} and \code{div} take the same form.\\

This is an important lesson on design, as a design principle, \textit{simplicity favours regularity}.

Consider the code

\begin{lstlisting}[language=C]
f = (g + h) - (i + j)
\end{lstlisting}

Compiles to the following MIPS Assembly

\begin{lstlisting}
add t0, g, h
add t1, i, j
sub f, t0, t1
\end{lstlisting}

\subsubsection{Register Operands}

Observe that the arithmetic instructions use register operands. MIPS has a 32x32-bit register file, that is, 32 register with 32-bit register size. Used for frequently accessed data, numbered 0 to 31. In Assembly, they are named \code{\$t0} to \code{\$t9} for temporary values and \code{\$s0} to \code{\$s7} for saved variables.\\

Smaller is faster.\\

In the same C code, consider that all variables are stored in \code{\$s0} to \code{\$s4}. 

(keep in mind that above MISP codes were not assemble-able).
\begin{lstlisting}
add $t0, $s1, $s2
add $t1, $s3, $s4
sub $s0, $t0, $t1
\end{lstlisting}

\subsubsection{Memory Operands}

Main memory is used for composite data (arrays, structures, dynamic data) to apply arithmatic operations (loading and storing values), it is byte-addressed. Words are alligned in memory (its address must be a multiple of 4.) [This design decision is shared with x86, this is why sometimes stuff is padded in C in weird ways.]\\

A \textbf{word} is a 4-byte construct, (32-bits) that correspond to an integer. Do consider that big-endianness or little-endianness of an architecture will impact in which order a word, or an integer, will be stored in the memory in which order.\\

MIPS is a big-endian architecture.[Meanwhile x86 is a little-endian architecture] \\

\begin{lstlisting}
lw d, off(b)
sw s, off(b)
\end{lstlisting}

\code{d} is the destination register, \code{b} is the base register \code{off} is offset  value, \code{b + off} forms the memory address to be accessed. \code{lw} is the \textbf{load word} instruction, while \code{sw} is the \textbf{store word} instruction.

Consider the C code \code{g = h + A[8];}, \code{g} is in \code{\$s1}, \code{h} in \code{\$s2} and the base address of the \code{A} array is in \code{\$s3}.

\begin{lstlisting}
lw $t0, 32($s3)
add $s1, $s2, $t0
\end{lstlisting}

Observe that the $8 \times 4 = 32$ since $4i$ is the offset value for a specific index value in C. This is due to the fact words occupy 4 bytes, and they align by four too.

Consider the similar C code \code{A[12] = h + A[8];} which also stores the result, this assembles into:

\begin{lstlisting}
lw $t0, 32($s3)
add $s1, $s2, $t0
sw $t0, 48($s3)
\end{lstlisting}

\missed \missed \missed

Since registers are significantly faster to access than memory, compiler should try to use registers for variables as much as possible. This would be correct even if register access time and memory access time was the same, [Which they aren't], since memory access requires more instructions.

\subsubsection{Immediate Operands}

Immediate instructions negate the need for using load instructions by using scalars. Consider, \code{addi} which is the immediate addition instruction, compared to \code{add}, it does not need the usage of a load. For instance, this snippet:

\begin{lstlisting}
li $t0, 32
add $s1, $s1, $t0
\end{lstlisting}

Can easilly be rewritten as

\begin{lstlisting}
add $s1, $s1, 32
\end{lstlisting}

Without using \code{li}. Which itself is an immediate operand that immediatelly loads a value to a register. This corresponds to the third design principle of MIPS, making the common case fast. Here, small constants are common, immediate operand avoids a load instruction.\\

Related to this, \code{\$zero} read-only register (it cannot be overwritten) holds the value zero, adding with the zero register can be used to move values between registers for instance.

\subsubsection{Logical Operations}

Logical operations are bitwise operations used to perform boolean logic on values. These are \code{and}, \code{or}, \code{nor}, and their immediate equivalents \code{andi} and \code{ori}.\\

And is useful for masking bits. For instance, \code{and}ing bits in a word with a word whose certain parts are zero would mask out these parts. Likewise, \code{or} is useful for swapping certain bits to 1 in a word.\\

MIPS does not have a not instruction, instead, one can \code{nor} (not or) with the special \code{\$zero} register. Since $\lnot p = \lnot (p \lor 0)$

 
\subsubsection{Shift Operations}

Another bitwise manipulation operations. \code{sll} and \code{srl} is used for left logical and right logical shifts respectivelly. Both of them fill the shifted parts with zero.\\

Shifting a number left $i$ times is same as multiplying it by $2^i$, and shifting a number $i$ times right would mean dividing it by $2^i$.

\subsection{Instruction Representation}

Instructions are kept as a series of high and low electronic signals. MIPS instructions are 32 bits long.\\

MIPS Instructions are classified into different types of instructions.

\subsubsection{R-Type Instructions}

\begin{table}[httb]
\begin{tabular}{@{}lll@{}}
Part & Length & Explanation\\
\toprule
\code{op} & 6 Bits & OP-Code\\
\code{rs} & 5 Bits & Source register\\
\code{rt} & 5 Bits & Source register\\
\code{rd} & 5 Bits & Destination register\\
\code{shamt} & 5 Bits & Shift Amount\\
\code{funct} & 6 Bits & Function Code\\
\bottomrule
\end{tabular}
\caption{The structure of R-Type Instructions}
\label{tab:rtype}
\end{table}

Most well-known of these R-Types are aritmathic instructions. Shift amount refeers to the amount of shift that will be applied to the number (For \code{sll} and \code{srl}.) Function code is used to further distinguish between instructions as an extension of the OP Code.\\

Register numbers in the machine codes differ from their assembler ccounterparts a bit. Registers between \code{\$t0} and \code{\$t7} are translated to numbers 8 to 15, Registers \code{\$t8} and \code{\$t9} gets translated to 24 and 25 and registers between \code{\$s0} and \code{\$s7} are translated to 16 to 23.\\

[I should probably point out the instructor at this point said we should now the general instruction structure but not the register translations.]\\

Function field may look redundant, it exists due to the fact that all arithmatic operations are encoded with OP-Code 0, hence they are differentiated via their Function codes. [This whole deal occurs to keep the OP-Code length the same between R, I and J type operations. There are many more R type operations, instead of giving OP-Code more space OP-Code field is used to group them to certain categories, and Function field specifies the exact instruction. This also greatly simplifies the processor design, all instructions with OP-Code 0 gets sent to ALU, for instance.]\\

[You may point out, but Amber, doesn't all of the R type instructions go to ALU anyway? What is the point? Well, padawan, MIPS is designed to be extended, Sony PSP for instance has an VFPU in it as well, and R Type instructions on VFPU use a different OP-Code]\\

Assembler instructions like \code{add}, \code{sub}, \code{div}, \code{srl} and \code{sll} are translated to R-Type instructions.

\subsubsection{I-Type Instructions}

\begin{table}[httb]
\begin{tabular}{@{}lll@{}}
Part & Length & Explanation\\
\toprule
\code{op} & 6 Bits & OP-Code\\
\code{rs} & 5 Bits & Register 1\\
\code{rt} & 5 Bits & Register 2\\
constant/address & 16 Bits & 16 Bit Number.\\
\bottomrule
\end{tabular}
\caption{The structure of I-Type Instructions}
\label{tab:itype}
\end{table}

Instructions in the I-Type are imediate arithmatic instructions, load/store instructions and branch instructions.\\

Meaning of registers change from instruction to instruction. The address is used to offset one of the registers.\\

Assembler Instructions like \code{addi}, \code{lw}, \code{sw} are translated to I-Type instructions.

\subsubsection{J-Type Instructions}

\begin{table}[httb]
\begin{tabular}{@{}lll@{}}
Part & Length & Explanation\\
\toprule
\code{op} & 6 Bits & OP-Code\\
\code{pseudo-address} & 26 Bits & Shortened address\\
\bottomrule
\end{tabular}
\caption{The structure of J-Type Instructions}
\label{tab:jtype}
\end{table}

[These were not mentioned, but I am still putting this here for the sake of completeness, although I believe we will return to these in the future.] \\

J-Type instructions are reserved for Jump instructions, hence they have a large amount of space left for an address. This value, stored at the \code{pseudo-address} field is the shortened address of the destibation, its two least significant and four most significant bits are removed and are assumed to be the same as the current instruction's address. (Wikibooks - MIPS Assembly)

\chapter{Instructions: Language of the Computer (Cont`d) - November 13, 2020}

This chapter directly continues the previous chapter.

\section{Instructions for Making Decisions}

\begin{lstlisting}
beq rs, rt, L1
bne rs, rt, L1
j L1
\end{lstlisting}

These instructions \textit{jump} to the label, labelled as \code{L1}. \code{beq} does this when two register values are equal, \code{bne} jumps when they are \textbf{not} equal, and \code{j} \textbf{unconditionally jumps} to the label \code{L1}.\\

\code{bne} is preffered over \code{beq}, this is because \textit{not branching} is preffered over branching for performance reasons.\\

\code{j} insrtuction is represented via J-Type instructions, while other branches are represented via I-Type.\\

\subsection{Basic Branching}

Branching instructions can be used to represent conditionals and loops easilly.

\subsubsection{Representing If-Else Conditionals Using Branching}


Consider the C code below:

\begin{lstlisting}[language=C]
if (i==j) f = g+h;
else f = g - h;
\end{lstlisting}

Assuming \code{f}, \code{g}, \code{h}, \C{i}, \C{j} is stored in \code{\$s0}, \code{\$s1}, \rs{2}, \rs{3} and \rs{4} respectivelly, this can be translated to:

\begin{lstlisting}
bne $s3, $s4, Else
add $s0, $s1, $s2
j Exit
Else: sub $s0, $s1, $s2
Exit: ...
\end{lstlisting}

Where under the label \C{Exit} will exit the program properly. This same program can also be written as:

\begin{lstlisting}
beq $s3, $s4, If
sub $s0, $s1, $s2
j Exit
If: add $s0, $s1, $s2
Exit: ...
\end{lstlisting}

\subsubsection{Representing Loops Using Branching}

A while loop of the form

\begin{lstlisting}[language=C]
while (save[i] == k)
	i += 1;
\end{lstlisting}

If \C{i}, \C{k} and the base address of save is stored in \rs{3}, \rs{5} and \rs{6} respectivelly, this is compiled to the following series of MIPS instructions:

\begin{lstlisting}
Loop: sll $t1, $s3, 2
	add $t1, $t1, $s6
	lw $t0, 0($t1)
	bne $t0, $s5, Exit
	addi $s3, $s3, 1
	j Loop
Exit: ...
\end{lstlisting}

Where shift left by two is used to multiply the index by four, the idea behind this decision is of course the elements are words, so we need to multiply the index by four to find the offset.

\subsubsection{Conditional Set}

\begin{lstlisting}
slt rd, rs, rt
slti rt, rs, constant
sltu rd, rs, rt
sltui rt, rs, constant
\end{lstlisting}

The \C{slt} instructions sets \C{rd} to 1 if the value in \C{rs} is less than \C{rt}, while \C{slti} does the same if the value in \C{rs} is less than the constant.\\

The \C{sltu} and \C{sltui} work the same, however they assume the numbers being hold at the registers are \textit{unsigned}. This could [or rather, \textit{will}] be problematic, given that signed integers work with two's complement.

\subsection{Procedure Calls}

Procedure calling works thusly:

\begin{enumerate}
\item Place parameters in registers.
\item Transfer control to procedure.
\item Acquire storage for procedure.
\item Perform procedure's operations.
\item Place result in register for caller.
\item Return to place of call. [Return control]
\end{enumerate}

\subsubsection{Register Structure}

\begin{table}[httb]
\begin{tabular}{@{}ll@{}}
Registers & Explanation\\
\toprule
\C{\$a0} - \C{\$a3} & To pass arguments\\
\C{\$v0}, \C{\$v1} & To store return values\\
\C{\$ra} & Return address\\
\C{\$sp} & Call stack pointer\\
\bottomrule
\end{tabular}
\caption{Registers used in procedure calls}
\label{tab:specialregisters}
\end{table}

\subsubsection{Procedure Call Instructions}


\begin{lstlisting}
jal ProcedureLabel
jr $ra
\end{lstlisting}

To call a procedure, we use the jump and link instruction, \C{jal}, the jump and link instruction stores the address of the next instruction (the adress the program will return to this address) [the return address] to the \Rra and jumps to the target address.\\

To return from a procedure, we use jump register instruction, \C{jr}, which copies the \Rra to the program counter, which means the next instruction to be executed will be the return address, that is, we jump back to where came. (Address of the last \C{jal} call + 1)

\subsubsection{Procedure Call}

\textbf{The caller} puts the parameter values in \Ra{0} - \Ra{3}, then uses \C{jal X} to cump to the procedure \C{X}, \C{jal} stores \C{PC+4} (Instructions are four bytes apart) in \Rra.\\

\textbf{The calee} performs the calculations (executes any instructions it has) places the results in \Rv{0} and \Rv{1}, it then returns the control to the caller using \C{jr \Rra}.\\

The PC (Program counter) is the specialised register containing the address of the instruction in the program being executed.\\

\subsubsection{Call Stack}

Any registers needed by the caller must be restored to the values that they contained before the procedure was invoked, for this reason, a stack is used, the special stack pointer \Rsp denotes the most recently allocated address in a stack that shows where registers should be spilled or where old register values can be found.

Consider the C code

\begin{lstlisting}[language=C]
int leaf_example (int g, int h,
  int i, int j) {
	int f;
	f = (g + h) - (i + j);
	return f;
}
\end{lstlisting}

Assume that arguments are stored in \Ra{0} - \Ra{3} and consider \C{f} in \Rs{0} (hence, \Rs{0} must be saved on stack). The C code \C{f = (g + h) - (i + j);} is translated to MIPS assembly:

\begin{lstlisting}
add $t0, $a0, $a1
add $t1, $a2, $a3
sub $s0, $t0, $t1
\end{lstlisting}

Since the callee will modify \Rt{0}, \Rt{1} and \Rs{0}. Their previous values (if they had any) must be stored in the call stack using \Rsp This is done by starting to add values to \Rsp by subtracting four times three from it, ie, allocating space for three registers.

\begin{lstlisting}
leaf_example:
	addi $sp, $sp, -12 # Allocate space.
	sw $t1, 8($sp) # Save stack.
	sw $t0, 4($sp)
	sw $s0, 0($sp)
	add $t0, $a0, $a1
	add $t1, $a2, $a3
	sub $s0, $t0, $t1
	add $v0, $s0, $zero # Store result.
	lw $t1, 8($sp) # Restore values
	lw $t0, 4($sp)
	lw $s0, 0($sp)
	addi $sp, $sp, 12
	jr $ra # Return
\end{lstlisting}

The above examples shows the complete MIPS translation of the \C{leaf\_example} function in C as a MIPS procedure.\\

[Stack pointer does not run out of space, because it is located in the top of the memory. It \textit{grows} and shrinks usable memory more calls you made to a function like this.]

\subsubsection{Convention}

This is not actually how most of the time MIPS programmers do things, instead, we assume that, across a procedure call, the following registers are preserved:

\begin{enumerate}
\item Saved registers, \Rs{0}-\Rs{7}
\item The stack pointer register \Rsp
\item The return address \Rra
\item Stack above the stack pointer, [ie, previous call].
\end{enumerate}

And the following isn't preserved:

\begin{enumerate}
\item Temporary, \Rt{0}-\Rt{7}
\item The argument registers \Ra{0}-\Ra{3}
\item The return registers \Rv{0}, \Rv{2}
\item Stack below the stack pointer, [ie, next calls, procedure calls made inside the procedure call].
\end{enumerate}

Using the convention, above code simplifies to:

\begin{lstlisting}
leaf_example:
	addi $sp, $sp, -4 # Allocate space.
	sw $s0, 0($sp) # Store saved.
	add $t0, $a0, $a1
	add $t1, $a2, $a3
	sub $s0, $t0, $t1
	add $v0, $s0, $zero # Store result.
	lw $s0, 0($sp)
	addi $sp, $sp, 2
	jr $ra # Return
\end{lstlisting}

If we haven't used the saved register \Rs{0}, then we didn't needed to save and restore it at all.\\

(From November 20, 2020)

\subsubsection{Nested Procedures}

Procedures that call other procedures, for nested call, caller needs to save its return address, and any arguements and temporaries needed after the call to the stack, and restore them from the stack after the call.\\

Consider the C code:

\begin{lstlisting}[language=C]
int fact (int n) {
	if (n < 1) return 1;
	else return n * fact(n - 1);
}
\end{lstlisting}

This is a very simple (and well-know, recursive) factorial algorithm, with the argument \C{n} being stored in \Ra{0} and result in \Rv{0}, this can be compiled to the following MIPS assembly code:

\begin{lstlisting}
fact:
	slti $t0, $a0, 1
	beq $t0, $zero, L1
	addi $v0, $zero, 1
	jr $ra	# and return
L1:
	addi $sp, $sp, -8
	sw $ra, 4($sp)
	sw $a0, 0($sp)
	addi $a0, $a0, -1
	jal fact
	lw $a0, 0($sp)
	lw $ra, 4($sp)
	addi $sp, $sp, 8
	mul $v0, $a0, $v0
	jr  $ra
\end{lstlisting}

Here, in the recursive part, we need to store the return address and the \Ra{0} in the stack to recover them later. We recover these values after the recursive call \C{jal fact}.\\

This is because, of course, if the values of \Ra{0} gets lost, then we will have significant problems, the code will not work correctly, and we do need to overwrite \Ra{0}, since the function we are calling, the function we are already in, expects its input value in \Ra{0}.\\

Original \Ra{0} may not be always have to stored however, since in certain recursive function we may not need to use the previous inputs, it is fine to overwrite them then.\\

Keep in mind there is a certain point that may arise if one uses the stack too much, if the stack size exceeds that of the memory, a stack overflow will occur.

[A similar problem occured in eraly Sinclair ZX80 computers, the screen memory was shared with the RAM, and if your program consumed too much memory, the display would literally shrink. It was reportedly not amusing to use Sinclair ZX 80 due to this reason, well, one of the reasons.]


\chapter{Instructions: Language of the Computer (Cont`d) - November 20, 2020}

\section{Representing Instructions}

Kept as a series of high and low electronic signals (binary) and represented as numbers. MIPS instructions are 32 bits long.\\

We had already gone over the I-Type and R-Type instructions extensivly. It should be noted that I-Type instructions also include the branching instructions, which does make sense, since they require two registers to compare and an address to jump to.

\subsubsection{J-Type Instructions}

\begin{table}[httb]
\begin{tabular}{@{}lll@{}}
Part & Length & Explanation\\
\toprule
\code{op} & 6 Bits & OP-Code\\
\code{pseudo-address} & 26 Bits & Shortened address\\
\bottomrule
\end{tabular}
\caption{The structure of J-Type Instructions}
\label{tab:jtype}
\end{table}

J-Type instructions are reserved for \C{j} and \C{jal} unconditional jump instructions. Observe that the Address can be any unsigned number up to $2^{28}$, compare to I-Type's $2^{16}$

\section{Addressing Modes}

There are different sorts of addressing modes used in MIPS.

\subsection{Immediate Addressing}

The operand is a constant withing the instruction itself. This is used in I-Type instructions with a constant, such as in \C{addi}. This value is a 16-bit value.

\subsubsection{32-Bit Immediate Operands}

But, let us say we want to load a very big 32-bit number such as 4000000, this is done via the \C{lui rt, constant} instruction, which sets the first 16 bits (upper part) of the register \C{rt} to the \C{constant} and sets the remaining part to the zero.

\begin{lstlisting}
lui $s0, 61
ori $s0, $s0, 2304
\end{lstlisting}

We \textbf{load} the upper 16 bits of the value to the first 16 bits of the \Rs{0} register using the Load Upper Immediate instruction, and then, we use the Logical Or Immediate operation to simply mask the remaining lower 16 bits (but signed, so can be negative) of the value to the register.\\

\subsection{Register Addressing}

Register addressing simply refeers to putting the number of the register to the register fields of the I-Type and R-Type instructions.

\subsection{Base Addressing}

The operand is at the memory location whose address is the sum of a register and a constant in the instruction, such as in, do realise that the offset might be negative as well:

\begin{lstlisting}
lw $t0, 1200($t1)
\end{lstlisting}

Which loads an integer from the 1200 bytes after the address at \Rt{1}. (\C{1200 + [\Rt{1}]}).

\subsection{PC-Relative Addressing}

The branch address is the sum of the PC and a constant in the instruction. \C{PC + offset x 4}

\begin{lstlisting}
beq rs, rt, L1
\end{lstlisting}

This is a relative addressing mode, this way, the amount of memory usable by the program increases substentially.\\

The multiplication by four part refeers to the fact that an instruction occupies four bytes (32-bits) in the memory.\\

\subsection{Pseudo-direct Addressing}

The jump address is the 26 bits of the instruction concatenated (after being multiplied by four) with the upper bits of the PC.

\begin{lstlisting}
j L1
\end{lstlisting}

Here the 26 bits represent the 28 lower bits of the address (it will be multiplied by the four, or shifted by two) and are concanated with the upper four bits of the program counter.

For instance, the following MIPS assembler instructions,

\begin{lstlisting}
Loop: sll $t1, $s3, 2
	add $t1, $t1, $s6
	lw $t0, 0($t1)
	bne $t0, $s5, Exit
	addi $s3, $s3, 1
	j Loop
Exit: ...
\end{lstlisting}

Compiles to these machine instructions:

\begin{table}[ht]
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
80000 & 0 & 0 & 19 & 9 & 2 & 0\\
\hline
80004 & 0 & 9 & 22 & 9 & 0 & 32\\
\hline
80008 & 35 & 9 & 8 & \multicolumn{3}{c|}{0}\\
\hline
80012 & 5 & 8 & 21 & \multicolumn{3}{c|}{2}\\
\hline
80016 & 8 & 19 & 19 & \multicolumn{3}{c|}{1}\\
\hline
80020 & 2 & \multicolumn{5}{c|}{20000}\\
\hline
\end{tabular}
\end{table}

\subsection{Branching Far Away}

If the branching target is too far to encode with 16-bit offset, assembler rewrites the code:
\begin{lstlisting}
beq $s0, $s1, L1
\end{lstlisting}

Becomes:

\begin{lstlisting}
bne $s0, $s1, L2
j L1
L2:
	...
\end{lstlisting}

\chapter{Instructions: Language of the Computer (Cont`d) - November 27, 2020}

\section{Byte \& Halfword Operations}


\begin{lstlisting}
lb rt, offset(rs)
lh rt, offset(rs)
\end{lstlisting}

For loading and storing bytes and halfwords, \C{lb}, \C{lh}, \C{lbu}, \C{lhu}; \C{sb} and \C{sh} can be used to get a single byte/halfword from the memory.\\

\C{lb} and \C{lh} sign extends the value to 32 bits in \C{rt}, their unsigned counterparts \C{lbu}, \C{lhu} on the other hand, zero-extends the value to 32 bits.\\
 
For instance, if we have \C{F0} in the memory, \C{lb} would load \C{FFFFFFF0}, but \C{lbu} would load \C{000000F0}.


\begin{lstlisting}[language=C]
void strcpy (char x[], char y[]) {
	int i;
	i = 0;
	while ((x[i]=y[i])!='\0')
		i += 1;
}
\end{lstlisting}

Assuming the addresses in \C{x, y} is in \Ra{0} and \Ra{1}. \C{i} in \Rs{0}.

\begin{lstlisting}
void strcpy (char x[], char y[]) {
	int i;
	i = 0;
	while ((x[i]=y[i])!='\0')
		i += 1;
}
\end{lstlisting}

\begin{lstlisting}
strcpy:
	addi $sp, $sp, -4
	sw $s0, 0($sp)
	add $s0, $zero, $zero
L1: 
	add $t1, $s0, $a1
	lbu $t2, 0($t1)
	add $t3, $s0, $a0
	sb $t2, 0($t3)
	beq $t2, $zero, L2
	addi $s0, $s0, 1
	j L1
L2:
	lw $s0, 0($sp)
	addi $sp, $sp, 4
	jr $ra
\end{lstlisting}

\section{Translation and Starting a Program}

\subsection{Compiler}

Compiler transforms higher level programming languages into assembly language program, a symbolic form of what the machine understands. [Some Compilers translate directly to machine code too.]

\subsection{Assembler}

Assembler converts Assembly instructions to machine code. Most of the time, machine instructions have a one-to-one relationship with machine instructions. \textbf{Pseudoinstructions} are instructions that do not have implementations, but simplify translation and programming (in Assembly) For instance \C{blt} pseudoinstruction assembles into a \C{slt} and \C{bne} instruction.\\

The \C{\$at} register is used as the assembler temporary to write this pseudoinstructions.\\

Assembler turns the program into an Object file that includes machine language instructions, data, and information needed to place instructions properly in memory.

\subsection{Object File}

The object file includes:

\begin{description}
\item[Object File Header] The size and position of the other pieces of the object file.
\item[Text Segment] The machine language code.
\item[Static Data Segment] Data allocated for the life of the program.
\item[Relocation information] Instructions and data words that depend on absolute addresses.
\item[Symbol Table] Global definitions and external references (labels)
\item[Debug Information] Associates machine instructions with C source files. [In \C{gcc}, you can enable this via the \C{-g3} switch.]
\end{description}

\subsection{Linker}

Places code and data modules symbolically in memory. Determines the addresses of data and instruction labels (by using relocation info) patches the internal and external references and produces executable file.

\subsection{Memory Layout}

In MIPS architecture, on top of the reserved segment sits the text segment, pointed to by the program counter, on top of which sits the \textbf{static data}, pointed by the \C{\$gp}, on top of which sits the \textbf{dynamic data}, the heap. and on top sits the stack, pointed by the \C{\$sp}.\\

Text segment and data segment sits in the object file, stack and dynamic data provided by the system itself. [In MS-DOS MASM x86, you actually had to tell the system the size of the stack you  wished.] Keep in mind that the stack grows \textit{downwards}, towards the dynamic data segment.

\subsection{Loader}

This is the part of the computer that actually copies the instructions and date from the executable file into memory. Copies the parameters to the main program, initializes the machine registers and sets the stack pointer to the first free locations, jumps to the start up routine that copies the parameters into the argument registers and calls the main routine.

\subsection{Dynamic Linking}

With Static linking, the library routines become part of the executable, loads all routines in the library even if not used, dynamically linked libraries (DLLs) are linked during execution.

\section{Compiler Optimization}

Many compilers do not just generate assembler code, in the background, certain optimizations are done to speed up code.

\subsubsection{gcc Optimization Levels}

\C{gcc}, the de facto standard GNU/Linux compiler provides six different optimization levels.\\

\begin{table}[ht]
\begin{tabular}{lll}
Switch & Optimization Level\\
\toprule
\C{-O0} & Optimize for compile time (default)\\
\C{-O1}, \C{-O} & first level of optimization size and execution time.\\
\C{-O2} & Second level of optimization size and execution time.\\
\C{-O3} & Third level of optimization for code size and execution time.\\
\C{-Os} & Optimize for code size\\
\C{-Ofast} & Optimizes even more than \C{-O3} but math is not IEEE compliant.\\
\bottomrule
\end{tabular}

\caption{Different optimization levels of GNU C Compiler.}
\label{tab:gccoptimization}
\end{table}

As can be seen in Table \ref{tab:gccoptimization}, there are different levels of optimization in \C{gcc}, these may increase the compilation time and memory usage, but in general will decrease the execution time. \C{-Ofast} allows for fast mathematical operations, but it is not completely IEEE compliant.

\subsection{Compiler Optimizations}

Here are certain optimizations performed by the compiler.

\subsubsection{Register Allocation}

Given a block of code, the compiler wants to choose assignments of variables to register to minimize the total number of required registers.

\begin{lstlisting}[language=C]
w = a + b;
x = c + w;
y = c + d;
\end{lstlisting}

Here the naive register allocation would result in assigning seven registers for the seven variables, but this is not actually necessary, there is a better way.

\subsubsection{Dead Code Elimination}

Code that will never be executed can be safely removed from the program.

\subsubsection{Loop Transformations}

For the processors, loops come with overhead, therefore, the compilers try to limit them.\\

First loop transformation is \textbf{loop unrolling.}

\begin{lstlisting}[language=C]
for (i = 0; i < N; i++) {
	a[i] = b[i] * c[i];
}
\end{lstlisting}

If the \C{N = 4}, then it can be unrolled four times.

\begin{lstlisting}[language=C]
a[0] = b[0] * c[0];
a[1] = b[1] * c[1];
a[2] = b[2] * c[2];
a[3] = b[3] * c[3];
\end{lstlisting}

Or it may be unrolled "twice", reducing the times we check the condition.:

\begin{lstlisting}[language=C]
for (i = 0; i < 2; i++) {
	a[i*2] = b[i*2] * c[i*2];
	a[i*2 + 1] = b[i*2 + 1] * c[i*2 + 1];
	
}
\end{lstlisting}

Another transformation is \textbf{loop fusion}, which combines two or more loops into a single loop.

\begin{lstlisting}[language=C]
for (i = 0; i < 300; i++) {
	a[i] = a[i] + 3;
}

for (i = 0; i < 300; i++) {
	b[i] = b[i] + 3;
}
\end{lstlisting}

is transferred to:
\begin{lstlisting}[language=C]
for (i = 0; i < 300; i++) {
	a[i] = a[i] + 3;
	b[i] = b[i] + 3;
}
\end{lstlisting}

Another transformation is \textbf{loop tiling}, which breaks up a loop into a set of nested procedures that operate on a subset of data.

\subsubsection{Procedure Inlining}

The body of the procedure is substituted in place for the procedure call to eliminate the call overhead.\\

This however, increases the code size. [CPython does not do this, it is one of the reasons it has abysmal performance compared to PyPy.]

\subsection{Performance Optimization}
While optimizing, it is pivotal to optimize at multiple levels, taking into account the data representation, algorithm, procedures and loops.\\

One must also understand the system to optimize performance, how programs are compiled and executed, how modern processors and memory systems operate, how to measure program performance and identify bottlenecks, how to improve performance without destroying code modularity and generality.

\end{document}